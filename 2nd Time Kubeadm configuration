Steps for Installing Kubernetes on CentOS 7
To use Kubernetes, you need to install a containerization engine. Currently, the most popular container solution is Docker. Docker needs to be installed on CentOS, both on the Master Node and the Worker Nodes.

==============================================================================================================
To Install Docker follow the below steps either links or commands: 

-----------------------------------------------------------------
Note: Do this steps on both K8s Master Node and Worker Nodes
-----------------------------------------------------------------

https://docs.docker.com/engine/install/centos/

Always examine scripts downloaded from the internet before running them locally.

$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh

[root@k8sn1 ~]# usermod -aG docker student

sudo systemctl enable docker && systemctl start docker
sudo systemctl status docker

[root@k8sn1 ~]# systemctl enable docker 
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.
[root@k8sn1 ~]# systemctl start docker 
[root@k8sn1 ~]# systemctl status docker 

Docker service must be running:
---------------------------------------------------------------------------------------
[root@k8s-workernode1 ~]# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2020-05-08 17:08:04 IST; 2h 14min ago
     Docs: https://docs.docker.com
 Main PID: 1102 (dockerd)
    Tasks: 10
   Memory: 627.1M
   CGroup: /system.slice/docker.service
           └─1102 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

May 08 17:08:01 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:01.523615086+05:30" level=info msg="Loading containers: start."
May 08 17:08:03 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:03.447762771+05:30" level=info msg="Default bridge (docker0) is assig...address"
May 08 17:08:04 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:04.117540405+05:30" level=info msg="Loading containers: done."
May 08 17:08:04 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:04.568552978+05:30" level=info msg="Docker daemon" commit=afacb8b gra...=19.03.8
May 08 17:08:04 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:04.568771664+05:30" level=info msg="Daemon has completed initialization"
May 08 17:08:04 k8s-workernode1 dockerd[1102]: time="2020-05-08T17:08:04.770021420+05:30" level=info msg="API listen on /var/run/docker.sock"
May 08 17:08:04 k8s-workernode1 systemd[1]: Started Docker Application Container Engine.
May 08 18:56:41 k8s-workernode1 dockerd[1102]: time="2020-05-08T18:56:41.677643744+05:30" level=info msg="ignoring event" module=libcontain...kDelete"
May 08 18:56:43 k8s-workernode1 dockerd[1102]: time="2020-05-08T18:56:43.396687218+05:30" level=info msg="ignoring event" module=libcontain...kDelete"
May 08 18:56:57 k8s-workernode1 dockerd[1102]: time="2020-05-08T18:56:57.959134191+05:30" level=info msg="ignoring event" module=libcontain...kDelete"
Hint: Some lines were ellipsized, use -l to show in full.
[root@k8s-workernode1 ~]# 

==============================================================================================================
Note: These three steps (1, 2, 3) are very important:
-----------------------------------------------------

1) Disable the swap:

free -m

swapoff -a

vim /etc/fstab

#/dev/mapper/centos-swap swap                    swap    defaults        0 0

mount -a

2) Disable the SELINUX:

setenforce 0;sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config

3) Stop & Disable the firewalld:

systemctl stop firewalld && systemctl disable firewalld

==============================================================================================================

Step 1: Configure Kubernetes Repository
Kubernetes packages are not available from official CentOS 7 repositories. This step needs to be performed on the Master Node, and each Worker Node you plan on utilizing for your container setup. Enter the following command to retrieve the Kubernetes repositories.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

==============================================================================================================
Note: If using the sudo command, append it not only to the cat command but to the restricted file as well.

Step 2: Install kubelet, kubeadm, and kubectl
These 3 basic packages are required to be able to use Kubernetes. Install the following package(s) on each node:
==============================================================================================================
sudo yum install -y kubelet kubeadm kubectl

systemctl enable kubelet
systemctl start kubelet

==============================================================================================================
You have now successfully installed Kubernetes, including its tools and basic packages.
=====================================================
kubeadm init 
=====================================================
[suresh@k8smaster ~]$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors="IsDockerSystemdCheck"
W0508 18:01:47.803694    4485 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.2
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
[suresh@k8smaster ~]$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors="IsDockerSystemdCheck","FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
W0508 18:02:54.389799    4644 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.18.2
[preflight] Running pre-flight checks
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
	[WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8smaster kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.122.187]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8smaster localhost] and IPs [192.168.122.187 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8smaster localhost] and IPs [192.168.122.187 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W0508 18:04:27.377739    4644 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W0508 18:04:27.379894    4644 manifests.go:225] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.504422 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.18" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8smaster as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8smaster as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: lik8qm.i4cms4uexn7llvoh
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.187:6443 --token lik8qm.i4cms4uexn7llvoh \
    --discovery-token-ca-cert-hash sha256:20944b52abab5884199dcba8cfd11069ae6340359b5fe0a579663ad5be864edc 
[suresh@k8smaster ~]$ 
================================================================================================================
Note: Core DNS pod won't be running 

Have a look into below outputs:

[suresh@k8smaster ~]$ kubectl get pods -A
NAMESPACE     NAME                                READY   STATUS    RESTARTS   AGE
kube-system   coredns-66bff467f8-nc45c            0/1     Pending   0          15m
kube-system   coredns-66bff467f8-rgwbm            0/1     Pending   0          15m
kube-system   etcd-k8smaster                      1/1     Running   0          15m
kube-system   kube-apiserver-k8smaster            1/1     Running   0          15m
kube-system   kube-controller-manager-k8smaster   1/1     Running   0          15m
kube-system   kube-proxy-gzzhf                    1/1     Running   0          15m
kube-system   kube-scheduler-k8smaster            1/1     Running   0          15m
[suresh@k8smaster ~]$
--------------------------------------------------------------------------------------------------------------------

Bring Core DNS or any other pods running we need to create CNI Network using CNI Plugins:

We'll use Calico CNI Plugin here:

To setup CNI - pod Network:

wget https://docs.projectcalico.org/v3.11/manifests/calico.yaml

Note:  Find the Value and replace "192.168.0.0/16" with "10.244.0.0/16"

[root@k8smaster ~]# cat -n calico.yaml |grep -i "10.244.0.0/16" 
   626	              value: "192.168.0.0/16"

Edit it using vi or vim:
------------------------
[suresh@k8smaster ~]$ vim calico.yaml

value: "10.244.0.0/16"

:wq 

Now create the CNI - pod Network:
---------------------------------

kubectl apply -f calico.yaml

Now, Check Pods Status:

As soon as you'll create CNI Pod Network you can notice the similar outputs:
[suresh@k8smaster ~]$ kubectl get pods -A
NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE
kube-system   calico-kube-controllers-75d56dfc47-xjjwr   0/1     Pending    0          5s
kube-system   calico-node-crvdh                          0/1     Init:0/3   0          5s
kube-system   coredns-66bff467f8-nc45c                   0/1     Pending    0          17m
kube-system   coredns-66bff467f8-rgwbm                   0/1     Pending    0          17m
kube-system   etcd-k8smaster                             1/1     Running    0          17m
kube-system   kube-apiserver-k8smaster                   1/1     Running    0          17m
kube-system   kube-controller-manager-k8smaster          1/1     Running    0          17m
kube-system   kube-proxy-gzzhf                           1/1     Running    0          17m
kube-system   kube-scheduler-k8smaster                   1/1     Running    0          17m
[suresh@k8smaster ~]$

=================================================================================================================
To join K8s Worker Node:

follow the below steps and commands:

[root@k8s-workernode1 ~]# kubeadm join 192.168.122.187:6443 --token lik8qm.i4cms4uexn7llvoh \
>     --discovery-token-ca-cert-hash sha256:20944b52abab5884199dcba8cfd11069ae6340359b5fe0a579663ad5be864edc 



Check below output, the similar lines would be seen 
-----------------------------------------------------

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[root@k8s-workernode1 ~]#

==============================================================================================
To lable K8s Worker Node:
--------------------------------------------------------------
Before Labeling the Worker Node:

[student@k8sm1 ~]$ kubectl get  nodes
NAME    STATUS   ROLES    AGE     VERSION
k8sm1   Ready    master   20m     v1.18.2
k8sn1   Ready    <none>   2m54s   v1.18.2
[student@k8sm1 ~]$ 
--------------------------------------------------------------
kubectl label node k8sn1 node-role.kubernetes.io/worker=worker

After Labeling the Worker Node:

[student@k8sm1 ~]$ kubectl get  nodes
NAME    STATUS   ROLES    AGE     VERSION
k8sm1   Ready    master   21m     v1.18.2
k8sn1   Ready    worker   3m39s   v1.18.2
[student@k8sm1 ~]$
--------------------------------------------------------------









